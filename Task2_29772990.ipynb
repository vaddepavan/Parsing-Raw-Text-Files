{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIT5196 Assessment 1 \n",
    "\n",
    "Student Name: VADDE PAVAN KUMAR \n",
    "\n",
    "Student ID: 29772990 \n",
    "\n",
    "Date: 14/04/2019\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the packages\n",
    "import re \n",
    "import PyPDF2 \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ********Extracting the data from the PDF *******\n",
    "\n",
    "units = [] # list to append all the data coming from PDF and storing in the list \n",
    "unit_code = [] #extracting the unit codes, from the PDF and storing them in the list \n",
    "unit_info = [] # extracting synopsis and outcomes and storing them in the list \n",
    "\n",
    "pdfFileObj = open('29772990.pdf', 'rb')   # Opening the PDF file to read the data and storing it in a variable \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)# using the package \n",
    "length = pdfReader.numPages\n",
    "for i in range(length):\n",
    "    pageObj = pdfReader.getPage(i)  # reading the page by page \n",
    "    word_data = pageObj.extractText() # reading the Text from the pages\n",
    "    units.append(word_data)# appendinbg the extracted text to the units list \n",
    "# here we are performing these steps to extract the unitcodes and unit information \n",
    "l1 = [] \n",
    "for unit in units:\n",
    "    l1.append(unit.split(\"\\n\"))  #splitting the strings with the respect to new line and storing in the list \n",
    "text = \"\"\n",
    "for u in l1:\n",
    "    for l2 in u: # we are jsut ignoring these headings in to the list \n",
    "        if l2 == \"Title\" or l2 == \"Synopsis\" or l2==\"Outcomes\":\n",
    "            continue\n",
    "        elif re.match(r'^([A-Z]{3}[0-9]{4})',l2): # we are matching the unitcodes \n",
    "            if len(l2) == 7: # there are many other units in the text so getting only the unit codes \n",
    "                unit_code.append(l2)\n",
    "                if len(text) > 0: # appeding units infformation to the list unit_info \n",
    "                    unit_info.append(text) #appending the text to the list \n",
    "                    text = \"\"\n",
    "            else:\n",
    "                if len(l2) > 0: # we are performing the normalization and converting the first word of each line to lower case \n",
    "                    l = l2.split(\" \")\n",
    "                    l[0] = l[0].lower() # picking the only first element and converting it to lower case \n",
    "                    l2 = ' '.join(str(elem) for elem in l) #we are rejoining the strings back to make space at the end of each line \n",
    "                    if len(text) == 0:\n",
    "                        text = text + l2\n",
    "                    else:\n",
    "                        text = text + \" \"+ l2\n",
    "        else: # same process is repeated, if there is no match of unitcode and then will be stored into unit info \n",
    "            if len(l2) > 0:\n",
    "                l = l2.split(\" \")\n",
    "                l[0] = l[0].lower()\n",
    "                l2 = ' '.join(str(elem) for elem in l)\n",
    "                if len(text) == 0:\n",
    "                    text = text + l2\n",
    "                else:\n",
    "                    text = text + \" \"+ l2             \n",
    "                \n",
    "unit_info.append(text) #This we are appending outside the loop because, its the last units info\n",
    "\n",
    "# ************Tokenizing and removing the stop words (Context independent)***************\n",
    "\n",
    "unigram_tokens_list = []\n",
    "stopped_tokens = []\n",
    "stem_tokens = []\n",
    "for info in unit_info: \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\") # Tokenizing the units info with the given regex \n",
    "    unigram_tokens = tokenizer.tokenize(info) \n",
    "    unigram_tokens_list.append(unigram_tokens) # tokenized words are stored into the list unit wise \n",
    "    \n",
    "    stopped_token = []\n",
    "    with open(\"stopwords_en.txt\") as fp: # removing the stop words as for the given stop words file \n",
    "        stop_words = fp.read().splitlines()\n",
    "        for w in unigram_tokens: \n",
    "            if w not in stop_words: \n",
    "                stopped_token.append(w) # if word not in the stop words then we are appending to the stooped tokens list\n",
    "        stopped_tokens.append(stopped_token)\n",
    "\n",
    "# *************Removing context dependent stop wordsand the tokens with the length  less than 3 **********\n",
    "\n",
    "high = []\n",
    "low = []\n",
    "cleaned_tokens = []\n",
    "list1=[item for sublist in stopped_tokens for item in sublist] # this counter caluclate sthe frquencies of each word \n",
    "a = dict(Counter(list1)) # Converting the counter lsit to the dict object \n",
    "for key in a.keys():  # fectching all the keys \n",
    "    if a[key] > 190: # we are checking whether the frequency of any data is greater than 190\n",
    "        high.append(key) # we are appending the word to the list named high \n",
    "    elif a[key] < 10:  # we are checking whether the frequency of any data is less than 10\n",
    "        low.append(key)  # we are appending the word to the list named low for low frequencied words \n",
    "# we are having 2 high frequency words ('unit', 'research')\n",
    "# we are having of 3641 words with less frequency \n",
    "merge = high + low # now we are merging both the lists to single lists so we can remove the high frequency and low frequency words \n",
    "for token in stopped_tokens: # now we are removing the things from the stopped tokes \n",
    "    clean = [] \n",
    "    for w in token: # iterating word by word\n",
    "        if w not in merge: # if the word of the unit is in the list then it will be excluded \n",
    "            if len(w) > 3: # and also checking the condition whether the token length is greater than 3 \n",
    "                 clean.append(w)\n",
    "    cleaned_tokens.append(clean)\n",
    "\n",
    "# *******************Applying porter stemmer to the tokens ******************\n",
    "\n",
    "stemmer = PorterStemmer() # intializing the PorterStemmer to the stemmer as short form \n",
    "final_conv_tokens =[] # list to store alll the final stemmed tokens \n",
    "### write your code below\n",
    "for s in cleaned_tokens: # getting the list of words from cleaned tokens \n",
    "    conv_tokens = []# stemmed tokens are being stored in list unit wise\n",
    "    for w in s:\n",
    "        cw = stemmer.stem(w) # stemming each word in the unit using porter stemmer\n",
    "        conv_tokens.append(cw)\n",
    "    final_conv_tokens.append(conv_tokens) # storing all the stemmed tokes unit wise into the final_conv_tokens \n",
    "\n",
    "flattened_list = [y for x in final_conv_tokens for y in x] # merging all the lists into single list for finding bigram measures \n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures() # importing bigram measure and bigram finder \n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(flattened_list) # importing the igram finder \n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams\n",
    "\n",
    "# we need to make sure those collocations are not split into two individual words\n",
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "colloc_info = [mwetokenizer.tokenize(info) for info in cleaned_tokens]\n",
    "\n",
    "# *******************Writing the Unigrams and bigrams to the file ********************\n",
    "\n",
    "combine_info = [j for i in colloc_info for j in i] # combining the lists into single list \n",
    "combine_info = set(combine_info)  # removing the duplicates from the list \n",
    "combine_info = sorted(combine_info) # sorted the words alphabetically \n",
    "save_file = open(\"29772990_vocab.txt\", 'w') # opening the file to write the data into file \n",
    "save_file.write(\"Unigram_Tokens\") # writing all the bigram tokens and appending the index number \n",
    "i = 0\n",
    "for t in combine_info:\n",
    "    save_file.write(t)\n",
    "    save_file.write(\":\")\n",
    "    save_file.write(str(i))\n",
    "    save_file.write(\"\\n\")\n",
    "    i = i+1\n",
    "\n",
    "save_file.write(\"\\n\")\n",
    "save_file.write(\"Bigram_Tokens\")\n",
    "for bigram in top_200_bigrams: # writing the top 200 bigrams to the file \n",
    "    save_file.write(str(bigram))\n",
    "    save_file.write(\"\\n\")\n",
    "save_file.close()\n",
    "\n",
    "# ************************** Writing the sparse representations to the file *************************\n",
    "\n",
    "save_file = open(\"29772990_countVec.txt\", 'w') # opening the file to write the vectors array \n",
    "vectorizer = TfidfVectorizer()\n",
    "for i in range(len(unit_code)):\n",
    "    # list of text documents\n",
    "    text = colloc_info[i]\n",
    "    # create the transform \n",
    "    vectorizer = CountVectorizer() # using the count vector to count the no frequency of each word in the unit info \n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit_transform(text) \n",
    "    # summarize\n",
    "    a = vectorizer.vocabulary_\n",
    "    save_file.write(unit_code[i]) # writing to the file as specified format\n",
    "    save_file.write(\",\")\n",
    "    for b in a.keys():\n",
    "        save_file.write(b)\n",
    "        save_file.write(\":\")\n",
    "        save_file.write(str(a[b]))\n",
    "        save_file.write(\",\")\n",
    "    save_file.write(\"\\n\")\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
